Policy Gradient for Portfolio Optimization
==========================================

The policy gradient (PG) method for portfolio optimization generates an action policy by performing a gradient ascent in a deep neural network that maximizes an objective function that is proportional to the agent's profit. According to the results obtained in `Liang et al. work <http://arxiv.org/abs/1808.09940>`_, the PG algorithm achieves higher performance than other reinforcement learning algorithms (such as DDPG and PPO) and is faster, since it only needs one neural network.

.. note::

    This algorithm works only with *PortfolioOptimizationEnv*, since it makes use of specific informations provided by the :code:`info` dictionary of the environment's :code:`step` function. 

Training Sequence
-----------------

In order to understand the training sequence, it is necessary to describe the two main structures utilized:

- The **replay buffer** is a structure that saves the experiences of the agent ordered in time. If the environment has :math:`N` simulation steps for episode, the replay buffer saves :math:`N` experiences, one for each time step. The experiences are composed of three `NumPy <https://numpy.org/>`__ arrays: the current observation, the last performed action and the variation of the prices of the assets.
- The **portfolio vector memory** (PVM) is a structure that keep track of the actions performed by the agent at each time step. If the environment has :math:`N` simulation steps for episode, the PVM saves :math:`N+1`: the vector memory has, as its first item, the last action to be used in the first simulation step and, thus, it needs to save an additional action. Actions are `NumPy <https://numpy.org/>`__ arrays and, initially, all the actions of the PVM are initialized as :math:`[1, 0, 0, \dots, 0]`.

Initially, a full episode is run (without training) in order to fill both replay replay buffer and the portfolio vector memory. Then, several training steps (the number depends on the training hyperparameters) are run with the following substeps:

1. A batch of sequential experiences is retrieved by the replay buffer. The retrieval is made by sampling a geometric distribution that favors more recent data: by setting the :code:`sample_bias` parameter, the user is able to define the probability of success of the distribution, which is the probability of choosing a sequential batch of data starting from the end of the replay buffer.

.. note::

    The user can change the behavior of the replay buffer sampling to favor older data by changing the parameter :code:`sample_from_start` to True.

2. With the batch of experiences in hand, the algorithm creates a batch of actions to be performed by the agent by performing a forward propagation in the policy neural network. Note that, since the policy :math:`\theta` requires the last performed action (:math:`A_{t} = \theta(S_{t}, A_{t-1})`), a batch of last actions is retrieved by the portfolio vector memory.

3. To simulate the effect of commission fees in the objective function, a batch of transaction remainder factors (:math:`\mu`) is generated by using the following equation

.. math::
    